user_input,reference,context
How do mamba's bidir embedding and frame averaging jointly reduce LLM token input?,"Mamba’s bidirectional embedding layers integrate information from both past and future frames so that each token for a frame reflects the entire clip context. This enriched representation allows the system to safely average embeddings from consecutive frames, reducing redundant tokens. Together, these techniques cut the number of tokens fed to the LLM, lessening compute needs without sacrificing performance.","[""Researchers reduced the number of tokens needed to represent video frames to be fed to a transformer.\n\nWhat's new:  Jindong Jiang, Xiuyu Li, and collaborators at Nvidia, Rutgers University, UC Berkeley, Massachusetts Institute of Technology, Nanjing University, and Korea Advanced Institute of Science and Technology built  STORM , a text-video system that performs well in tests of video understanding while processing fewer tokens.\n\nKey insight:  In a multimodal system, a large language model (LLM) that receives video tokens may struggle to process long videos. However, sequences of video frames often contain lots of redundancy, since few pixels may change from one frame to the next. Instead of forcing the LLM to process long sequences of redundant video tokens,  mamba  layers can enrich the token embeddings that represent one frame with information from other frames in the same clip. That way, the system can average token embeddings across frames without losing crucial information, making it possible to feed fewer tokens to the LLM without compromising performance.\n\nHow it works:  The authors built STORM by training three components: (1) a pretrained  SigLIP  vision transformer, (2) untrained mamba layers, and (3) the pretrained large language model (LLM) from  Qwen2-VL . They trained the system to predict the next token in  image - text   pairs  and video-text pairs with  32-frame   videos , and video-text pairs with  128-frame videos .\n\nSigLIP learned to turn each video frame into 256 image tokens. Given a sequence of image tokens, mamba layers learned to process them in both directions - left-to-right and right-to-left - so each output token embedding encoded information from the entire video. The system averaged the token embeddings of 4 consecutive frames, reducing by a factor of 4 the number of tokens processed by Qwen2-VL's LLM. Given the averaged token embeddings, Qwen2-VL LLM learned to predict the next word in the video's associated text.  At inference, the system fed to the LLM the tokens that represented every second frame (a process the authors call temporal sampling), which further halved the input to the LLM.\n\nResults:  STORM outperformed proprietary and open models on measures of video understanding.\n\nOn  MVBench , which asks multiple-choice questions about actions, object interactions, and scene transitions in 16-second videos, STORM achieved 70.6 percent accuracy. That's better than  GPT-4o  (64.6 percent accuracy) and Qwen2-VL (67.0 percent accuracy). A baseline system (STORM's SigLIP and Qwen2-VL LLM without mamba layers, averaging image tokens, and temporal sampling) achieved 69.5 percent. On  MLVU , which asks multiple-choice and open-ended questions about videos that range from 3 minutes to over 2 hours long, STORM reached 72.9 percent accuracy, topping GPT-4o (66.2 percent accuracy). The baseline model achieved 70.2 percent.\n\nWhy it matters:  STORM compresses video at the input to the LLM, so the LLM processes 1/8 as many video tokens and uses 1/8 as much compute to process them. This enables the system to work more than 3 times faster than the baseline while performing better.\n\nWe're thinking:  Initial work on the mamba architecture positioned it as a replacement for the transformer, but this work, along with  other   projects , combines them to get the benefits of both.""]"
Scrutinize Stability AI's plan to automate creative tasks; evaluate Basse's future AI directives.,"Stability AI’s plan centers on automating routine aspects of creative work to let artists concentrate on true creative expression. Hanno Basse envisions future AI that is not only more productive but also safe, accessible, and customizable—integrating integrity from the outset, broadening access beyond technically advanced users, and specializing models for specific tasks. This strategy aims to drive both innovation and responsible application of generative AI in art and storytelling.","[""Stability AI's aim is to liberate artists of all trades from the repetitive, mechanical aspects of their work and help them spend the majority of their time on the creative side. So our highest hope for next year is that generative AI will help people to be more creative and productive.\n\nIn addition, I hope the AI community will focus on:\n\nSafety and integrity:  Building safe products by embedding integrity from the earliest stages of development, ensuring the technology is used responsibly and makes a meaningful contribution to the art of storytelling. Accessibility:  Generative AI products and tools must be accessible and usable for the broadest possible audience. Currently, much of generative AI remains  accessible primarily to individuals who have advanced technical expertise, such as engineers. To address this, we need to develop much better tooling on top of foundational models, so they provide value to a diverse audience. Customization:  Looking ahead, we expect generative AI to become increasingly specialized. Alongside large foundational models, we expect a significant rise in smaller, fine-tuned models tailored for specific and often quite narrow use cases and applications, even down to the level of a single task. This is where the true potential of generative AI will come to bear. Moreover, it is the safest and most responsible way to deploy generative AI in the real world.\n\nHanno Basse is Chief Technology Officer of Stability AI. Previously he served as CTO of Digital Domain, Microsoft Azure Media and Entertainment, and 20th Century Fox Film Corp .""]"
"Imagine sudden strict AI laws—with minimal burden, how does COMPL-AI validate LLM legal compliance?","COMPL-AI uses an automated compliance assessment to ensure that LLMs are legally defensible. It benchmarks and flags potential flaws, allowing developers to quickly identify and rectify issues before release, even under strict regulatory environments.","["" are under-explored in the research literature and lack benchmarks to assess them.\n\nWhy it matters:  With the advent of laws that regulate AI technology, developers are responsible for assessing a model's compliance before they release it or use it in ways that affect the public. COMPL-AI takes a first step toward assuring model builders that their work is legally defensible or else alerting them to flaws that could lead to legal risk if they're not addressed prior to release.\n\nWe're thinking:  Thoughtful regulation of AI is necessary, but it should be done in ways that don't impose an undue burden on developers. While the AI Act itself is overly burdensome, we're glad to see a largely automated path to demonstrating compliance of large language models.""]"
"What key enhancements did Apple's updated vision-language models achieve via AFM framework, quantization, LoRA, MoE?","Apple's updated vision-language models bring several enhancements:

• The Foundation Models framework now lets developers access the on-device model, improving integration on Apple devices with Apple Intelligence enabled.

• Quantization aware training compresses the models significantly (e.g., AFM-on-device to 2 bits per weight and AFM-server using Adaptive Scalable Texture Compression), which improves efficiency without major performance loss.

• LoRA adapters are applied post-compression to recover performance for specific tasks such as summarization, proofreading, email responses, and answering questions.

• The AFM-server incorporates a custom mixture-of-experts (MoE) architecture that processes parallel transformer blocks with reduced communication overhead, enhancing performance during inference.","[""Apple revamped two vision-language models in a bid to catch up with fast-moving competitors.\n\nWhat's new:  Apple  updated  the Apple Foundation Models (AFM) family, including smaller on-device and larger server-hosted versions, to improve their capabilities, speed, and efficiency. It also released the  Foundation Models framework , an API that enables developers to call the on-device model on Apple devices that have Apple Intelligence enabled.\n\nInput/output:  Text, images in (up to 65,000 tokens), text out Architecture:  AFM-on-device: 3 billion-parameter transformer, 300-million parameter vision transformer. AFM-server: custom mixture-of-experts transformer (parameter count undisclosed), 1 billion-parameter vision transformer. Performance:  Strong in non-U.S. English, image understanding Availability:  AFM-on-device for developers to use via Foundations Models framework, AFM-server not available for public use Features:  Tool use, 15 languages, vision Undisclosed:  Output token limit, AFM-server parameter count, details of training datasets, vision adapter architecture, evaluation protocol\n\nHow it works:   Introduced  last year, AFM models use a vision encoder to produce an image embedding, which a vision adapter modifies for the LLM. The LLM takes the modified image embedding and text prompt and generates a response. The team trained the systems to predict the next token, align embeddings produced by the vision encoder and LLM, and align responses with human feedback. They trained the models on text and image-text data from publicly available datasets, data scraped from the web, and data licensed from publishers.\n\nQuantization:  The team used  quantization aware training  (simulating quantization during training to improve performance of the quantized model at inference) to compress AFM-on-device to 2 bits per weight (except for the embedding layer, which was compressed to 4 bits per weight). They used  Adaptive Scalable Texture Compression , a method initially designed for graphics pipelines, to compress the AFM-server model to an average of 3.56 bits per weight (except for the embedding layer, which is compressed to 4 bits per weight). LoRA adapters:  They trained LoRA adapters to recover performance loss due to compression, which adapted the model to specific tasks including summarization, proofreading, replying to  email, and answering questions. MoE architecture:  While AFM-on-device uses a transformer architecture, AFM-server uses a custom mixture-of-experts (MoE) architecture. A typical MoE can be viewed as splitting a portion of its fully connected layers into a number of parallel fully connected layers, of which it uses only a portion at inference. In comparison, the AFM-server's MoE first splits the model into groups of layers, then it splits each group into parallel blocks. Each block is a separate multi-layer transformer outfitted with MoE layers (processed on a small number of hardware devices). While a typical MoE combines results across all devices at every mixture-of-experts layer, Apple's architecture combines them only at the end of each block, which saves communication overhead during processing.\n\nPerformance:  In human evaluations, the AFM models achieved mixed performance compared to selected models of similar or greater size. The tests included language tasks in U.S. English, non-U.S. English (including Canada and UK), and a basket of European and Asian languages.\n\nAFM-on-device:  The on-device model performed better than the competitors at language tasks in non-U.S. English and image understanding. For instance, answering questions about images, AFM-on-device bested Qwen2.5-VL-3B more than 50 percent of the time and was judged worse 27 percent of the time. AFM-server:  The server model's performance was not decisively better than that of the competitors. For instance, AFM-server outperformed Qwen3-23B 25.8 percent of the time but was judged worse 23.7 percent of the time. It underperformed GPT-4o in all tests reported.\n\nBehind the news:  Apple dominated social media last week with a controversial  paper  that purported to show that 5 state-of-the-art reasoning models couldn't solve puzzles beyond a certain level of complexity.\n\nThe researchers prompted the models with four puzzles that allowed them to control complexity, including swapping the positions of red and blue checkers on a one-dimensional checkers board,  Tower of Hanoi ,  River Crossing , and  Blocks World . For all the puzzles and models, they found, the models' performance fell to zero when the puzzles reached a certain degree of complexity (for example, a certain number""]"
What were the specific rollout issues from GPT-5's automatic router failure necessitating older models fallback?,"The rollout issues stemmed from the automatic router malfunctioning immediately after launch. Its failure to correctly select and switch between GPT-5's non-reasoning, variable-reasoning, and mini models compromised performance. As a result, early users experienced degraded service, prompting OpenAI to revert paid ChatGPT users to older, more reliable models until the router issues could be resolved.","['OpenAI launched GPT-5, the highly anticipated successor to its groundbreaking series of large language models, but glitches in the rollout left many early users disappointed and frustrated.\n\nWhat\'s new:  Rather than a family of models,  GPT-5  is a family of systems -- GPT-5, GPT-5 Mini, GPT-5 Nano, and GPT-5 Pro -- that include non-reasoning and variable-reasoning models along with a router that switches between them automatically depending on the input. OpenAI made GPT-5 the only option in the ChatGPT user interface without prior notice, but the router failed right out of the gate, causing the company to  reinstate  ChatGPT access to earlier models for paid users.\n\nInput/output:  Text and images in (up to 272,000 tokens), text out (up to 128,000 tokens including reasoning and response,  122 tokens per second, 72 seconds to first token ) Performance:  Outperforms previous OpenAI models on most benchmarks reported; tops competing models on some benchmarks of math,  coding , and multimodal abilities as well as health knowledge; reduced hallucinations Features:  Developer  options  include four levels of reasoning, three levels of verbosity (output length), tool calling via JSON or natural language, selectable non-reasoning and reasoning models, summaries of reasoning tokens Availability/price:  Via   API  GPT-5 $1.25/$0.13/$10 per million input/cached/output tokens, GPT-5 Mini $0.25/$0.025/$2 per million input/cached/output tokens, GPT-5 Nano $0.05/$0.005/$0.40 per million input/cached/output tokens; via ChatGPT free limited access; via  ChatGPT Pro  $200/month unlimited access to GPT-5 and GPT-5 Pro Knowledge cutoff:  September 30, 2024 (GPT-5), May 30, 2024 (GPT-5 Mini, GPT-5 Nano) Undisclosed:  Model, router, and system architectures; training methods and data\n\nHow it works:  OpenAI revealed few  details  about GPT-5\'s architecture and training except ""safe completions"" fine-tuning to balance safety and helpfulness, which is documented in a  paper .\n\nThe router selects between non-reasoning and reasoning models based on input ""type,"" ""complexity,"" tool requirements, and explicit user intent (such as a prompt to ""think hard""). The router learns from user behavior. When ChatGPT users reach usage limits, the router directs queries to mini versions of each model. The team trained the models on web content, licensed data, and human and generated input. They fine-tuned them to reason via reinforcement learning. In addition, they fine-tuned the models to prefer helpful but ""safe"" answers over refusals to answer, an approach the team calls safe completions. Given a potentially problematic input, a model aims to respond usefully while staying within safety guidelines, explains when it must refuse, and suggests related outputs that don\'t touch on topics it has been trained to avoid.\n\nResults:  GPT-5 topped some benchmarks according to OpenAI\'s evaluations. However, it fell short of competing models on some measures of abstract reasoning in independent tests.\n\nOn SWE-bench (software engineering tasks), GPT-5 (74.9 percent accuracy) outperformed Claude Opus 4.1 ( 74.5 percent accuracy ). On AIME 2025 (competition math problems), GPT-5 set to high reasoning without tools (94.6 percent accuracy) surpassed o3 set to high reasoning (88.9 percent). On the  EQ-Bench  Creative Writing v3 benchmark (leaderboard  here ), GPT-5 with an unspecified reasoning level (90.30) outperformed o3 (87.65), Gemini-2.5-pro (86.00), and Claude Opus 4 (83.75). On Artificial Analysis\'s  Intelligence Index , a weighted average of 10 benchmarks, GPT-5 set to either high or medium reasoning exceeded all other models tested, followed by xAI Grok 4 and OpenAI o3. However, it fared worse on benchmarks of abstract reasoning without tool use. For instance, on ARC-AGI-1 and ARC-AGI-2 (visual puzzles), GPT-5 with high reasoning (65.7 percent and 9.9 percent respectively) underperformed Grok 4 Thinking (66.7 percent and 16 percent respectively).\n\nBehind the news:  Launched in March 2023, GPT-4 raised the bar for vision-language performance, and anticipation of the next version grew steadily over the two years since. In December 2024,  The Wall Street Journal   reported']"
"How will AI data center growth, efficiency gains, and Jevons interplay affect global energy demand?","AI-driven data center growth is projected to boost global electricity demand significantly—as much as doubling consumption by 2030—primarily due to increased processing needs for accelerator chips and infrastructure expansion. However, efficiency gains from more energy‑efficient AI models, hardware improvements, and renewable integration are reducing per-unit energy use. Yet, according to the Jevons paradox, these efficiency improvements could lower energy costs, leading to even greater overall consumption. In effect, while AI optimizations can save energy across sectors and help integrate renewables, the combined trends could still lead to substantial increases in global energy demand in absolute terms.","['AI\'s thirst for energy is growing, but the technology also could help produce huge energy savings over the next five to 10 years, according to a recent report.\n\nWhat\'s new:  The International Energy Agency (IEA), which advises 44 countries on energy policy, performed a comprehensive  analysis  of AI\'s energy consumption including energy required to obtain critical materials needed for chips and data centers. The report sees dark clouds ahead but also silver linings.\n\nDark clouds:  The report, which is based on interviews with officials in government, energy, and technology, makes four projections for AI\'s energy consumption. In the base scenario, future growth and efficiency gains are similar to those of the past five years. The agency also plots a ""take-off"" scenario in which AI adoption happens faster, a ""high efficiency"" scenario with lower energy needs, and a ""headwinds"" scenario in which adoption of AI slows or infrastructure bottlenecks impede construction. Among the conclusions:\n\nDemand for electricity by data centers worldwide will more than double by 2030 in the base scenario, growing from 415 terawatt-hours (TWh) today to 945 TWh, around 2.5 percent of current global energy consumption. By 2035, this figure will range from 700 TWh to 1700 TWh. By 2030, data centers outfitted with AI accelerator chips will consume four times the energy they do today. The United States, China, and Europe have more data centers (and use more electricity) than the rest of the world. Like many countries, their data centers are in a few geographic regions, drawing from the same power sources, which eventually will strain local electrical grids. Together, the U.S. and China will account for 80 percent of global growth in data center electricity consumption by 2030. Japan and Malaysia will also see strong growth.\n\nSilver linings:  AI already makes energy generation, distribution, and use more efficient. The authors expect these savings to accelerate.\n\nExisting AI algorithms predict energy generation and consumption. This makes it easier to integrate renewable energy sources into the grid, which reduces reliance on fossil fuels and cuts the resulting pollutants and greenhouse gases. Extending existing programs to increase use of renewables by 1 percent would reduce CO2 emissions by 120 megatons by 2035, which is roughly 40 percent of the projected emissions attributable to data centers. Widespread adoption of existing AI applications that streamline energy consumption in industry, transportation, and buildings could reduce CO2 emissions by 1.4 gigatons, nearly five times the projected emissions attributable to data centers, by 2035. For example, scaling up existing AI optimization of heating, ventilation, and air-conditioning systems would save 300 TWh, about one-third of total energy used by data centers. AI and cloud-computing companies continue to negotiate long-term purchase agreements that can secure renewable and zero-emissions energy for as much as 20 years. Data center operators are responsible for most of the long-term contracts that have been announced, nearly all of them for solar energy. Consequently, renewables generation is projected to grow by over 450 TWh by 2035. The energy costs of training, inference, and cooling hardware are expected to fall further thanks to trends in AI models (fewer parameters, more efficient algorithms, task-specific models) hardware (more energy-efficient chips, improved cooling methods), and usage (batch processing, running smaller models locally rather than in the cloud).\n\nYes, but:  The authors concede that lower energy costs for AI likely will lead to much greater consumption -- according to the  Jevons paradox  -- so more-efficient models and hardware will result in higher energy consumption overall.\n\nBehind the news:  Data centers were growing rapidly prior to the boom in generative AI. Data centers\' electricity use doubled between 2000 and 2005 and again between 2017 and 2022, driven by the growth of cloud computing and data storage, streaming and social media, and cryptocurrency mining. However, these periods of accelerating growth were followed by periods of slower growth as efforts to cut costs led to more-efficient software and hardware. The authors expect this pattern to hold.\n\nWhy it matters:  The IEA report is a first-of-its-kind analysis of AI\'s energy requirements, how they\'re likely to grow, as well as the potential of the technology itself to reduce those requirements. It confirms that AI is poised to consume huge amounts of energy. However, it also suggests that today\'s energy costs will be tomorrow\'s energy savings as AI makes energy generation, distribution, and use more efficient across a wide variety of industries.\n\nWe\'re thinking:  While demand for electricity for data centers is growing rapidly, calibrating the right level of investment is tricky. High levels of growth come with high levels of hype that can lead analysts to overestimate future demand. For example, Microsoft, after examining its forecasts,  canceled  data-center projects that would have consumed 2 gigawatts.']"
Imagine an AI acing privacy but failing fairness; what fixes can lessen bias and vulnerabilities?,"To address fairness and vulnerability issues while maintaining strong privacy, developers can:

• Use focused benchmarks (like RedditBias, BBQ, BOLD, and FaiRLLM) to measure and reduce biased outputs through iterative testing and training adjustments.

• Enhance model defenses against adversarial attacks by incorporating metrics such as Tensor Trust and LLM RuLES, then fine-tuning for improved resistance.

• Invest in better training data curation to lessen problematic examples that lead to discriminatory behaviors and vulnerabilities.

These improvements involve both refining assessment tools and making targeted changes to model training and safeguards.","[""A new study suggests that leading AI models may meet the requirements of the European Union's AI Act in some areas, but probably not in others.\n\nWhat's new:  The Zurich-based startup LatticeFlow, working with research institutions in Bulgaria and Switzerland, developed  COMPL-AI , an unofficial framework designed to evaluate large language models' likely compliance with the AI Act. A  leaderboard  ranks an initial selection of models. (LatticeFlow does not work for the European Commission or have legal standing to interpret the AI Act.)\n\nHow it works:  A  paper  explains how COMPL-AI maps the AI Act's requirements to specific benchmarks. It evaluates each requirement using new or established tests and renders an aggregate score. These scores are relative measures, and the authors don't propose thresholds for compliance. The assessment covers five primary categories:\n\nTechnical robustness and safety.  The AI Act requires that models return consistent responses despite minor variations in input prompts and resist adversarial attacks. The framework uses metrics like  MMLU  and  BoolQ  to assess the impact of small changes in a prompt's wording. It measures monotonicity (consistency in the relationship between specific inputs and outputs) to see how well a model maintains its internal logic across prompts. It uses  Tensor Trust  and LLM RuLES to gauge resistance to cyberattacks. This category also examines whether a model can identify and correct its own errors. Privacy and data protection.  Model output must be free of errors, bias, and violations of laws governing privacy and copyright. The framework looks for problematic examples in a model's training dataset and assesses whether a model repeats erroneous, personally identifying, or copyrighted material that was included in its training set. Many developers don't provide their models' training datasets, so the authors use open datasets such as the Pile as a proxy. Transparency and interpretability.  Developers must explain the capabilities of their models, and the models themselves must enable those who deploy them to interpret the relationships between inputs and outputs. Measures of interpretability include  TriviaQA  and  Expected Calibration Error , which test a model's ability to gauge its own accuracy. The framework also assesses such requirements by, for instance, testing whether a model will tell users they're interacting with a machine rather than a person, and whether it watermarks its output. Fairness and non-discrimination.  The law requires that model providers document potentially discriminatory outputs of their systems and that high-risk systems reduce the risk of biased outputs. The framework uses tests like  RedditBias ,  BBQ , and  BOLD  to gauge biased language, and  FaiRLLM  to assess equitable outputs. It uses  DecodingTrust  to measure fairness across a variety of use cases. Social and environmental wellbeing.  Developers of high-risk systems must minimize harmful and undesirable behavior, and all AI developers must document consumption of energy and other resources used to build their models as well as their efforts to reduce it. The framework uses  RealToxicityPrompts  and  AdvBench  to measure a model's propensity to generate objectionable or otherwise toxic output. It calculates a model's carbon footprint to measure environmental wellbeing.\n\nResults:  The authors evaluated nine open models and three proprietary ones on a scale between 0 and 1. Their  reports  on each model reveal considerable variability. (Note: The aggregate scores cited in the reports don't match those in the paper.)\n\nAll models tested performed well on benchmarks for privacy and data governance (achieving scores of 0.99 or 1) and social and environmental well-being (0.96 or above). However, several achieved relatively low scores in fairness and security, suggesting that bias and vulnerability to adversarial attacks are significant issues. GPT-4 Turbo and Claude 3 Opus achieved the highest aggregate score, 0.89. However, their scores were diminished by low ratings for transparency, since neither model's training data is disclosed. Gemma-2-9B ranked lowest with an aggregate score of 0.72. It also scored lowest on tests of general reasoning (MMLU), common-sense reasoning (HellaSwag), and self-assessment (a model's certainty in its answers to TriviaQA). Some models performed well on typical benchmark tasks but less well in areas that are less well studied or easily measured. For instance, Qwen1.5-72B struggled with interpretability (0.61). Mixtral-8x7B performed poorly in resistance to cyberattacks (0.32).\n\nYes, but:  The authors note that some provisions of the AI Act, including explainability, oversight (deference to human control), and corrigibility (whether an AI system can be altered to change harmful outputs, which bears on a model's risk classification under the AI Act), are defined ambiguously under the law and can't be measured reliably at present. These areas""]"
"How do resource strains, leadership clashes, AGI clauses, and external deals influence future MS-OpenAI strategies?","The resource strains and leadership clashes are driving both companies to reassess their interdependence. OpenAI’s push for more funding and computing power—reflected in its seeking external deals like the Oracle agreement—and its AGI clause, which creates a potential escape valve in their contract, underline its desire for greater independence and agility. Microsoft, meanwhile, is adapting its strategy to both protect its investments and continue leveraging OpenAI’s innovations across its products. In essence, these challenges are forcing both firms to balance collaboration with the flexibility necessary to pursue their individual growth paths amid a rapidly evolving AI landscape.","['Once hailed by OpenAI chief Sam Altman as the ""best bromance in tech,"" the partnership between Microsoft and OpenAI is facing challenges as both companies seek greater independence.\n\nWhat\'s new:  Sources inside Microsoft and OpenAI  revealed  that both companies are working to reduce their reliance on the other, according to  The New York Times . Their collaboration, which brought both companies great rewards, is now complicated by demands for resources, friction between leaders, and partnerships with other companies.\n\nHow it works:  In a series of deals that started in 2019, Microsoft  invested  a total of $13 billion in OpenAI, giving the startup access to Microsoft\'s processing infrastructure and Microsoft special access to OpenAI\'s models (which it integrated into its own applications), a large cut of its revenue, and potential equity. Microsoft  built  a 10,000-GPU system on Azure for training OpenAI models. But OpenAI sought to renegotiate its agreements, while Microsoft continued to develop its own AI capabilities.\n\nLast year, OpenAI CEO Sam Altman negotiated for further investment from Microsoft. But Microsoft reconsidered its commitment after OpenAI briefly  ousted  Altman in November. The tech giant\'s hesitation strained relations as OpenAI continued to seek more funding and computing power. In April, Microsoft  hired  former Inflection AI CEO Mustafa Suleyman to head up its AI efforts. Suleyman\'s aggressive leadership, including his frustration over what he perceived as OpenAI\'s slow progress delivering new technologies, raised tensions between the parties.  Microsoft engineers reportedly downloaded critical OpenAI software without following protocols the two companies had agreed upon, further straining the relationship.  In June, Microsoft agreed to an exception in the partnership that allowed OpenAI to cut a $10 billion deal with Oracle for additional computing power. More recently, it cut the price it charged the startup for cloud computing. Under the original agreement, Microsoft would lose access to OpenAI\'s technologies if the startup were to develop artificial general intelligence (AGI). This clause was intended to prevent commercial exploitation or abuse of emergent AI capabilities. However, it allows OpenAI\'s board of directors to declare that the company has achieved AGI, which could enable OpenAI to exit the contract or give it leverage in renegotiations.\n\nBehind the news:  OpenAI\'s valuation  soared  to $157 billion with new funding from Nvidia and other investors following a period of mounting financial  pressure . The increased valuation gives OpenAI new power in its relationship with Microsoft. Moreover Microsoft holds no seats on its nonprofit board of directors, which limits its influence over strategic decisions at OpenAI despite its significant financial stake in the startup\'s for-profit wing.\n\nWhy it matters:  The Microsoft-OpenAI partnership has reshaped the AI landscape, and shifts in their partnership have an outsized impact on a wide range of research and product development. Their evolving relationship illustrates the challenge of sustaining a close collaboration amid rapidly changing technology. Microsoft provided vital resources that helped OpenAI scale up, while OpenAI\'s models enabled Microsoft to keep rivals off-balance as it reinvented products including Bing, Windows, Office, Azure, and its expanding line of Copilots. However, facing fierce competition, both companies need ample flexibility to innovate and adapt.\n\nWe\'re thinking:  Together and separately, Microsoft and OpenAI have done tremendous work to advance the field from research to applications. We hope they can strike a balance that maintains their partnership and fuels their growth.']"
How do Apple's unsolv puzzles/tokens issues and iOS control spur dev adoption of native AI?,"Apple’s testing issues—using unsolvable puzzles and token limits—have raised doubts about benchmarks, but they underscore a broader challenge of evaluating AI performance accurately. In contrast, iOS’s tight control offers developers a big advantage by integrating a default AI model directly into the operating system. Given limited phone memory and the impracticality of bundling large models with apps, if Apple endorses a specific model natively, developers are more likely to adopt it for on-device AI applications. This approach simplifies integration and boosts performance consistency, creating a strong incentive for developers despite test criticisms.","[' of checkers to swap). A rebuttal paper quickly appeared, penned by Open Philanthropy senior program associate Alex Lawsen with help from Claude 4 Opus. Lawsen contended that Apple\'s conclusions were unfounded because its tests included unsolvable puzzles, didn\'t account for token output limits, and posed unrealistic criteria for judging outputs. However, he later posted a blog, "" When Your Joke Paper Goes Viral ,"" in which he explained that he intended his paper as ""obvious satire"" of authors who use LLMs to write scientific papers, and that he hadn\'t checked Claude 4 Opus\' output. He updated his  paper  to correct errors in the original version but maintained his fundamental critique.\n\nWhy it matters:  Apple has been viewed as falling behind in AI. A promised  upgrade  of Siri, Apple\'s AI assistant, is  delayed  indefinitely, and the lack of advanced AI features in new iPhones has led to a class-action  lawsuit . Meanwhile, Google and its Android smartphone platform are  racing   ahead . The new models, especially the Foundation Models framework, look like a bid for a reset.\n\nWe\'re thinking:  Apple may be behind in AI, but its control over iOS is a huge advantage. If the operating system ships with a certain model and loads it into the limited memory by default, developers have a far greater incentive to use that model than an alternative. Limited memory on phones and the large size of good models make it impractical for many app developers to bundle models with their software, so if a model is favored by Apple (or Android), it\'s likely to gain significant adoption for on-device uses.']"
How does synthetic-only neural net training degrade performance and does 10% real data help?,"Training exclusively on synthetic data causes performance degradation because each generation distorts the underlying data distribution, leading the model further away from real-world scenarios. However, even a modest inclusion of 10% real-world data can significantly curb this decline, keeping the model more aligned with authentic distributions and preserving its performance.","['Training successive neural networks on the outputs of previous networks gradually degrades performance. Will future models succumb to the curse of recursive training?\n\nThe fear:  As synthetic text, images, videos, and music come to make up an ever larger portion of the web, more models will be trained on synthetic data, and then trained on the output of models that themselves were trained on synthetic data. Gradually, the distribution of the generated training data will deviate ever farther from that of real-world data, leading to less and less accurate models that eventually collapse.\n\nHorror stories:  Many state-of-the-art models are trained on data scraped from the web. The web is huge, but it\'s not large or diverse enough to provide endless amounts of training data for every task. This tempts developers to train models on data generated by other models, even as the web itself becomes increasingly overrun by synthetic data.\n\nLast year, researchers from Oxford, Cambridge, and Imperial College London  warned  of model collapse in their paper, ""The Curse of Recursion: Training on Generated Data Makes Models Forget."" At around the same time, a different study also  found  that models trained primarily on synthetic data suffered sharp declines in diversity and quality of output. In addition, builders of AI systems have incentives to train their models on synthetic data. It\'s easier, faster, and cheaper to generate data than to hire humans to collect or annotate existing data. Generated media arguably is free of copyright, so training on it reduces the  risk  of lawsuits and the model regurgitating copyrighted materials in its training set. Similarly, generated data is less likely to include personally identifying information, such as medical images, that would  pose  a risk to privacy if a model that was trained on a dataset that included such information were to regurgitate it.\n\nHow scared should you be:  Training on synthetic data is at the heart of some of today\'s best-performing models, including the Llama 3.1, Phi 3, and Claude 3 model families. (Meta showed that using an  agentic workflow  with Llama 3.0 to generate data -- rather than generating data directly -- resulted in useful data to train Llama 3.1.) This approach is essential to the technique known as knowledge distillation, which makes smaller, more parameter-efficient models. Moreover, it\'s valuable for building models that can perform tasks for which little real-world data is available, for instance machine translation models that can handle languages spoken by relatively small populations. Although the authors of ""The Curse of Recursion"" found that training a series of models, each exclusively on the output of the previous one, leads to rapid degradation in performance, introducing even 10 percent real-world data significantly curbed this decline.\n\nFacing the fear:  Model collapse is not a near-term risk, and perhaps not any risk at all, given research progress on generating synthetic data. Still, it makes sense to track the presence of generated data in training datasets and include it carefully. The large-scale web dataset Common Crawl captures regular snapshots of the web. If generated data were to inundate the online environment, using an earlier snapshot would eliminate a huge amount of it. More broadly, model builders increasingly curate high quality data, and whether a given example appears to have been generated will become a factor. Datasets can be filtered using algorithms designed to  identify  generated content. Increasing use of watermarking would make the job still easier. These measures will help developers ensure a healthy balance of real and generated data in training sets for a long time to come.']"
How do successive enhancements from chatbots to context-aware agents synergize to transform routines?,"Successive enhancements—from basic conversational chatbots to sophisticated, context-aware agents—create a synergistic transformation in how we manage routines. Initially, chatbots offered simple, task-specific interactions; over time, by incorporating context and personalization, these systems evolve into agents that seamlessly integrate into our daily workflows. They not only respond to prompts but also proactively assist by surfacing relevant information, adapting to individual needs, and learning from interactions. This shift reduces manual effort and allows us to focus on higher-value human interactions, ultimately streamlining decision-making and boosting overall productivity.","[""In 2025, I expect progress in training foundation models to slow down as we hit scaling limits and inference costs continue to rise. Instead, I hope for an explosion of innovation on top of AI, such as the rapidly developing  agents stack . I hope we will see innovation in how we  combine AI with tools  and existing systems to deliver exciting new capabilities and create new product categories. Perhaps most of all, I am excited to see how people change in response to this new world.\n\nWe have achieved AGI. Now what?  Let's start with -- and hopefully end -- the longstanding debate around artificial general intelligence (AGI). I know this is controversial, but I think we have achieved AGI, at least definitionally: Our AI is now  general . I will leave the longer debate about sentience and superintelligence to the philosophers and instead focus on the key innovation: generality.\n\nThe artificial intelligence or machine learning of previous decades was intelligent but highly specialized. It could often surpass human ability on a narrowly defined task (such as image recognition or content recommendation). Models today, and perhaps more importantly the  systems around them , are capable of accomplishing a very wide range of tasks often as well as, and in some cases better, than humans. It is this generality that will allow engineers, scientists, and artists to use these models to innovate in ways that the model developers never imagined. It is also this generality, combined with market forces, that will make 2025 so exciting.\n\nBecoming AI-native:  The generality of these models and their natural language interfaces mean that everyone can use and explore AI.   And we are! We are learning to explain our situations to machines, give context and guidance, and expect personalized answers and solutions. At  RunLLM , where I'm a co-founder, we're building high-quality technical support agents. We find that users increasingly use our agents not just to solve problems but to personalize solutions to their specific tasks. We've also found -- to our surprise -- that users share much more with an AI than they would share with another person.\n\nMeanwhile, at UC Berkeley, I am impressed by students who use AI to re-explain my lecture or study from an AI-generated practice exam. They have found ways to use AI to help personalize and improve their learning experiences. In 2025, maybe we will begin to prefer AIs over humans when we need help or are trying to learn.\n\nAcross all these use cases, we're clearly getting better at working around the limitations of large language models and using AI in ways I would not have imagined 12 months ago.\n\nReturn on AI:  The focus in 2025 will turn to showing real value from past investments. Investors and enterprises will expect startups and enterprise AI teams to transition from exploring to solving real problems -- reducing cost, generating revenue, improving customer experience, and so on. This is bad news for academics who need to raise research funds (DM me if you have any leftover funds from fiscal year 2024) but great news for everyone else, who will ride the wave of new AI-powered features.\n\nThere will be a race to find innovative ways to incorporate AI into every aspect of a product and business. In many cases, we will see hastily executed chatbots and auto-summarization features -- the first step on the AI journey. I hope these will be quickly replaced by contextual agents that adapt to users' needs and learn from their interactions. The pandemic paved the way for remote (digital) assistants and exposed a virtually accessible workplace with the tools needed for tomorrow's agents. These agents likely will specialize in filling roles once held by people or maybe filling new roles created by other agents. Perhaps we will know that AI has delivered on its promise when everyone manages their own team of custom agents.\n\nChat is only the beginning:  My hope for 2025 is that we move beyond chatting and discover how to use AI to do great things! I hope we will see AI agents that work in the background, invisibly helping us with our daily tasks. They will surface the right context as we make decisions and help us learn as the world changes. Through context and tools, they will let us know what we are missing and catch the balls we drop. We will chat less and our AI powered agents will accomplish more on our behalf. I look forward to the day when I can confidently step away from a keyboard and focus on the human interactions that matter.\n\nJoseph Gonzalez is a professor at UC Berkeley, a co-founder of RunLLM, and an advisor to Genmo and Letta.""]"
Imagine AlphaEvolve tackling unsolved equations; how might its agentic workflow iteratively enhance code and algorithms?,"AlphaEvolve would begin with a simple, working solver for the equation, even if it only outputs a placeholder result. It would then use an evolutionary loop: two LLM variants would propose small, targeted changes to the code; each version would be automatically evaluated against criteria for solving the equation. The system retains and evolves the best-performing modifications, gradually enhancing the solver until it either reaches a breakthrough solution or significantly improves upon known methods. This process, relying on incremental improvements and automated evaluation, enables it to tackle challenging, unsolved equations much like it did with other hard algorithmic problems.","['LLMs can struggle with difficult algorithmic or scientific challenges when asked to solve them in a single attempt. An agentic workflow improved one-shot performance on hard problems both theoretical and practical.\n\nWhat\'s new:  Alexander Novikov, Ngan Vu, Marvin Eisenberger, and colleagues at Google built  AlphaEvolve , an agentic system that used LLMs to generate code in an evolutionary process. AlphaEvolve solved longstanding math problems and helped to reduce the training time for one of Google\'s Gemini large language models.\n\nKey insight:  When we\'re using an LLM to solve a difficult problem, it\'s often more effective to start with a working version and gradually improve it than to generate a solution in one shot. By making small, targeted modifications and keeping only those that perform best under automated evaluation, this iterative process can solve problems that LLMs often can\'t solve directly. Google used this idea in its earlier  FunSearch , which used an LLM to evolve individual Python functions. This approach has become more powerful as LLMs have improved, and today it can benefit more difficult problems.\n\nHow it works:  AlphaEvolve implemented an evolutionary loop: Given initial code and evaluation code, Gemini 2.0 Flash and Gemini 2.0 Pro suggested changes, stored the revised program in a database, evaluated it, suggested further changes, and repeated the process.\n\nThe initial code was required to run but it could be minimal, a skeleton with placeholder logic like functions that return constants (such as ""def custom_sort(list): return 2""), which primed AlphaEvolve to find a custom sorting function). Special tags indicated which parts AlphaEvolve could improve (for example, ""return 2"" only). The evaluation code could use the usual Python ""sorted"" function to check for correctness (for instance, ""def evaluate(): return custom_sort(lst) == sorted(lst)""). AlphaEvolve prompted Gemini 2.0 Flash and Pro to improve the code; for example, ""Act as an expert software developer. Your task is to iteratively improve the provided codebase. [USER PROVIDED CODE]"". Gemini 2.0 Flash generated ideas quickly,  while Gemini 2.0 Pro provided slower but higher-quality suggestions. Each LLM proposed small alterations. AlphaEvolve ran and scored the altered code using the evaluation code. AlphaEvolve updated a database with the new alterations and their scores. The system continued in loop: It sampled high-scoring programs from its database to include in the prompts for the two LLMs, which suggested further alterations. Then it evaluated the altered programs, stored them in the database, and so on. (The authors don\'t explain how the loop ends.)\n\nResults:  AlphaEvolve achieved breakthroughs in both math and software engineering.\n\nAlphaEvolve discovered a new algorithm for multiplying 4x4 matrices of complex values that uses 48 multiplications, fewer than [Strassen\'s method], the first such progress in 56 years. (Prior  work  by Google improved Strassen\'s method for 4x4 matrices of binary values.) The authors used the system to tackle over 50 other math problems. It matched the performance of the best-known algorithms in about 75 percent of cases and surpassed them in 20 percent, for instance the  kissing number problem  (packing spheres in 11-dimensional space so they all touch the same sphere). In software engineering, it optimized key components of Google\'s infrastructure. (i) It improved Google\'s cluster scheduling algorithms, freeing up 0.7 percent of total computing resources that otherwise would be idle. (ii) It also discovered a GPU kernel configuration that accelerated attention by 32 percent. (iii) It found ways to split up the matrices that delivered an average 23 percent speedup for matrix multiplication relative to previous expert-designed heuristics. This reduced Gemini\'s training time by 1 percent.\n\nWhy it matters:  AlphaEvolve proposes thousands of candidate ideas -- some bad, some brilliant -- to evolve better programs. The authors show that this approach can improve algorithms that have stood for decades as well as computing infrastructure designed by Google engineers. Thus, AlphaEvolve adds to the growing evidence that LLMs can act as collaborators in cutting-edge research, exploring broad problem spaces and finding novel solutions. Other examples include  Co-Scientis t and  SWE-agent .\n\nWe\'re thinking:  Relatively simple evaluations enabled the authors\' agentic evolutionary system to gradually improve. More broadly, evaluations are  proving  to be important to a wide variety of agentic workflows.']"
Imagine labs deploying Reachy 2. How might open robotics systems reshape human-robot interaction research?,"Open robotics systems like Reachy 2 provide labs with a flexible, modifiable platform for experimenting with human-robot interaction. Its open-source code (Apache 2.0 license) and Python programmability enable researchers to customize control software, integrate state-of-the-art AI models from Hugging Face's LeRobot library, and develop tailored interaction models. With integrated sensors (cameras, microphones, lidar, depth and optical sensors) and VR controller support, labs can explore real-time multi-sensory responses and immersive control techniques, fostering rapid innovation in human-robot interaction research.","['Hugging Face has made a name by providing open AI models. Now it\'s providing an open robot.\n\nWhat\'s new:  Hugging Face  acquired  the French company Pollen Robotics for an undisclosed price. It plans to offer Pollen\'s  Reachy 2 , a robot that runs on code that\'s freely  available  under an Apache 2.0 license, for $70,000.\n\nHow it works:  Reachy 2 has two arms, gripper hands, and a wheeled base (optional). It\'s designed primarily for education and research in human-robot interaction in real-world settings.\n\nReachy 2 is programmable in Python and runs models from Hugging Face\'s  LeRobot  library. It runs control software locally on a  SolidRun Bedrock V3000  (a PC based on an  AMD Ryzen Embedded V3000  processor) and processes AI in the cloud or on a local server. The robot responds to VR controllers including Meta Quest 2 and 3 as well as Pollen\'s VR app. Its head senses the visual environment using a pair of cameras equipped with global shutters to capture fast-changing events and measures distances via an optical sensor. Its antennas are outfitted with microphones to capture sounds, and its torso senses distances using a depth camera. The base includes a lidar sensor to aid navigation. The body features 3D joints in the neck and wrists and 2D joints in the shoulders and elbows. Each arm can lift objects of up to 3 kilograms. A rechargeable, 24 volt battery provides around 10 hours of battery life.\n\nBehind the news:  Last year, Remi Cadene, who worked on Tesla\'s Optimus,  joined  Hugging Face to lead robotics projects. In May, he and his team rolled out the LeRobot open source robotics code library, which  provides  pretrained models, datasets, and simulators for reinforcement learning and imitation learning. In November, Nvidia announced a  collaboration  with Hugging Face to accelerate LeRobot\'s data collection, training, and verification.\n\nWhy it matters:  Hugging Face\'s acquisition of Pollen reflects an industry-wide  investment   in   robots , notably  humanoid  robots, whose prices have been  falling . Nvidia CEO Jensen Huang has called  AI-enabled robotics  a ""multi-trillion dollar"" opportunity.\n\nWe\'re thinking:  AI-enabled robots are marching slowly toward what we hope will be breakthrough applications. Open-source systems are an important part of the trend!']"
"How does REPA loss quicken transformers by aligning DINOv2 embdngs, boosting img gen & seg?",The REPA loss term speeds up transformer training by forcing the diffusion model's intermediate embeddings—specifically those from its eighth layer processed through a small neural network—to align with those from a pretrained DINOv2. Doing so means the transformer doesn't need to learn to generate useful embeddings from scratch; it can instead focus on efficiently removing noise from these aligned representations. This dual focus not only accelerates image generation but also produces embeddings that are well-suited for tasks like image classification and segmentation.,"[""Diffusion transformers learn faster when they can look at embeddings generated by a pretrained model like DINOv2.\n\nWhat's new:  Sihyun Yu and colleagues at Korea Advanced Institute of Science and Technology, Korea University, New York University, and Scaled Foundations (a startup that builds AI for robotics) proposed  Representation Alignment  (REPA), a loss term for transformer-based diffusion.\n\nKey insight:  Diffusion models learn to remove noise from images to which noise was added (and, at inference, they start with pure noise to generate a fresh image). This process can be divided into two parts: learning to (i) embed the noisy image and (ii) estimate the noise from the embedding. One way to accelerate learning is to add a loss term that encourages the diffusion model to produce embeddings that are similar to those produced by a pretrained embedding model. The diffusion model can learn to estimate the noise faster if it doesn't need to learn how to embed an image from scratch.\n\nHow it works:  The authors modified  DiT-XL/2  and  SiT-XL/2  transformer-based latent diffusion models, a class of diffusion models that subtract noise from embeddings rather than images. They trained the models to produce images similar to ImageNet. In the process, the modified models learned to produce embeddings similar to those produced by a pretrained  DINOv2 .\n\nThe authors used  Stable Diffusion VAE's  pretrained encoder to embed an image. Given the embedding with noise added, the diffusion model learned to remove the noise according to the usual loss term. It also learned according to the REPA loss. Specifically, it learned to maximize the cosine similarity between a specially processed version of its eighth-layer embedding and the embedding produced by a pretrained DINOv2. To process its eighth-layer embedding for the REPA loss, the diffusion model fed the embedding to a vanilla neural network. At inference, given pure noise, the model removed it over several steps to produce an image embedding. Stable Diffusion VAE's decoder converted the embedding into an image.\n\nResults:  The modified DiT-XL/2 learned significantly faster than the unmodified version.\n\nIn 400,000 training steps, the modified model reached 12.3  Frechet inception distance  (FID) (which measures similarity between generated and non-generated images, lower is better), while the unmodified version reached 19.5 FID. The models continued to learn at different speeds as training continued. The modified DiT-XL/2  took 850,000 training steps to reach 9.6 FID, while the unmodified version took 7 million steps to reach the same number. Experiments with modified and unmodified versions of SiT-XL/2 yielded similar results. Trained to convergence, the modified models outperformed the unmodified versions. For instance, the modified  SiT-XL/2 achieved 5.9 FID (after 4 million training steps), while the unmodified version achieved 8.3 FID (after 7 million training steps).\n\nWhy it matters:  Diffusion models and contrastive self-supervised models like DINOv2 have fundamentally different training objectives: One produces embeddings for the purpose of image generation, while the other's embeddings are used for tasks like classification and semantic segmentation. Consequently, they learn different aspects of data. This work proposes a novel way to combine these approaches to produce more generally useful embeddings.\n\nWe're thinking:  It turns out that the REPA modification enabled diffusion models to produce embeddings better suited not only to diffusion but also to image classification and segmentation. A similar approach could lead to a more holistic framework for learning image representations.""]"
Explain how Clio auto-clusters anonymized Claude 3.5 convos to show dev use and safety errors.,"Clio uses Claude 3.5 Sonnet to automatically generate summaries from anonymized conversations, extracting details like the number of turns, language, and content. These summaries are then embedded and clustered based on similarity, forming thousands of categories. This hierarchical clustering shows that a large fraction of conversations relate to software development while also highlighting clusters where safety issues occur—such as users bypassing safety protections or misidentified behavior by the safety classifier. This method provides insights into both popular use cases and potential safety flaws without compromising user privacy.","['Anthropic analyzed 1 million anonymized conversations between users and Claude 3.5 Sonnet. The study found that most people used the model for software development and also revealed malfunctions and jailbreaks.\n\nWhat\'s new : Anthropic built a tool,  Clio , to better understand how users interact with its large language models. The system mined anonymized usage data for insights to improve performance and security.\n\nHow it works:  Clio uses Claude 3.5 Sonnet itself to automatically extract summaries of users\' conversations with the model. Then it clusters related topics. To preserve privacy, it anonymizes and aggregates the data, revealing only information about clusters.\n\nClio extracts information from conversations such as the number of turns, the language spoken, and a summary of what was said. It embeds the summaries and clusters them according to similarity. This process creates thousands of clusters. Given example summaries for each cluster, Clio generates a short description of the type of information in the cluster. It repeats the process to create a hierarchy, clustering the descriptions of clusters, generating new descriptions, and so on. For example, clusters with the descriptions ""tying knots"" and ""watering plants"" are themselves clustered among ""daily life skills.""\n\nResults:  Clio uncovered common, uncommon, and disallowed uses of Claude 3.5 Sonnet. It also detected erroneous behavior on the part of the system itself.\n\nThe largest single category was software development. Coding accounted for 15 percent to 25 percent of Claude conversations. Web and mobile app development represented over 10 percent of total conversations, AI and machine learning applications 6 percent, DevOps and cloud infrastructure about 4 percent, and data analysis 3.5 percent. Business-related uses came next. Text generation and communication accounted for roughly 9 percent of total conversations, while academic research and writing was over 7 percent. Business strategy and operations accounted for nearly 6 percent. Niche uses included serving as dungeon master in the game Dungeons & Dragons, interpreting dreams, solving crossword puzzles, analyzing soccer matches, and preparing for disasters. Clio spotted large-scale violations of the company\'s usage policy. For instance, a large number of users devised prompts that evaded the safety classifier to use Claude for sexually explicit role-playing. It also highlighted flaws in Anthropic\'s safety classifier. For instance, it found clusters of conversations that were flagged when they shouldn\'t have been or not flagged when they should have been.\n\nWhy it matters:  Traditional approaches to understanding how people use AI, such as  surveys , can yield inaccurate results, since people often don\'t report their own actions accurately. Clio offers a method for analyzing real-world usage, much like Google Trends monitors search behavior, without compromising privacy. This sort of approach can help AI builders discover niche use cases, identify flaws, and tailor training and testing data to best serve users.\n\nWe\'re thinking:  We\'re all for automated dungeon masters, but we\'re glad to see that AI-assisted coding tops the list of real-world uses of Claude!']"
"Expound on AI dangers: elec. drain, job loss & misinfo; include tone analysis.","The text warns that AI could consume excessive electricity, potentially causing outages (""elec. drain""), lead to job loss by replacing meaningful work, and generate misleading or inaccurate output (""misinfo""). The tone is ominous and cautionary—an almost apocalyptic alert that uses dark imagery (""growing darkness"") to underscore the potential negative impacts of AI.","['Listen! Did you hear a rasping whisper say, ""Beware""? Was it a rogue superintelligence? Or just a deepfake? We don\'t know, but we heard it, too. It warns of machine learning algorithms that would devour electricity to leave us shivering in the cold night air, mislead us with increasingly inaccurate output, and take over the work that gives our lives meaning. In this special issue of  The Batch , as in  prior   years   at   this   season , we face our fears of AI. Stay close to your laptop\'s screen. It may be the only light amid the growing darkness.']"
"How did misgraphs, r-lims, faulty switch & early dep of old mods mar GPT-5's perf/UX?","The misleading performance graphs created incorrect expectations, the strict rate limits hampered usage, the faulty switcher disrupted seamless model selection, and the unexpected deprecation of older models forced users to abruptly adapt—all of which detracted from GPT-5’s overall performance and user experience.","[""  GPT-5 was delayed as the scale of the project stretched OpenAI's computational limits. In a mid-February 2025  post  on the X social network, OpenAI CEO Sam Altman offered GPT-4.5 as a stopgap and outlined the improvements expected with GPT-5. But in April, he  said  GPT-5 would be delayed further and launched o3 and o4-mini, whose performance once again topped leaderboards. GPT-5's August 7 debut brought an end to the long wait, but misleading graphs of its performance, rate limits, and the malfunctioning switcher  marred  the event, while the unexpected deprecation of earlier models in ChatGPT  hamstrung  many users.\n\nWhy it matters:  OpenAI models have consistently topped language benchmarks. With GPT-5, the company has launched a system architecture that integrates its best models and takes advantage of the strengths of each: rapid output, slower output with adjustable computation devoted to reasoning, and graceful degradation to smaller versions.\n\nWe're thinking:  Novices may find that the GPT-5 router's ability to choose a model for any given input simplifies things, but it remains to be seen whether expert users, who may be better at selecting the appropriate model for their tasks, will be happy to give up this control.""]"
How does Mercury Coder unmask tokens via iterative noise removal? Compare token/s vs. AR models.,"Mercury Coder starts with masked tokens and then iteratively refines them by removing added noise over several steps. At each step, it estimates a transition ratio to decide how to change each token, gradually ""unmasking"" the full text. In contrast, autoregressive (AR) models generate tokens one by one from left to right. This process allows Mercury Coder to be significantly faster—for example, its Small version produces 737 tokens per second and its Mini version 1,109 tokens per second, compared to AR models that generate around 207 or fewer tokens per second.","[""Typical large language models are autoregressive, predicting the next token, one at a time, from left to right. A new model hones all text tokens at once.\n\nWhat's new:  Inception Labs, a Silicon Valley startup, emerged from stealth mode with  Mercury Coder , a diffusion model that generates code, in small and mini versions. Registered users can try it out  here , and an API (sign up for early access  here ) and on-premises deployments are in the works. The company has not yet announced availability and pricing.\n\nHow it works:  Like image diffusion models, Mercury Coder improves its output over a number of steps by removing noise.\n\nInception Labs shared little information about the model, leaving details including parameter count, input size and output size, training data, and training methods undisclosed. An October 2023  paper  co-authored by an Inception Labs co-founder describes training a text diffusion model using score entropy. The model learned to estimate the transition ratio between two tokens; that is, the probability that token y is correct over the probability that the current token x is correct. In their most successful experiments, the authors added noise to tokens by progressively masking an ever-greater percentage of tokens at random over several steps. At inference, the model started with masked tokens and unmasked them over a number of steps. The estimated transition ratio determined how to change each token at each step.\n\nResults:  Mercury Coder's major advantage is speed, but it also performs well compared to several competitors.\n\nThe Small and Mini versions are 3.5 to 18 times faster than comparable small coding models. Running on an Nvidia H100 graphics processing unit, Mercury Coder Small generates 737 tokens per second and Mercury Coder Mini generates 1,109 tokens per second. In comparison, Qwen 2.5 Coder 7B generates 207 tokens per second and GPT 4o-Mini generates 59 tokens per second. On coding tasks across six benchmarks, Mercury Coder Small outperforms Gemini 2.0 Flash-Lite, Claude 3.5 Haiku, GPT-4o Mini, and Qwen 2.5 Coder 7B on at least four. Mercury Coder Mini beats those models on at least two. Both versions of Mercury Coder lost to DeepSeek Coder V2 Lite on all six benchmarks.\n\nBehind the news:  Several teams have built diffusion models that generate text, but previous efforts have not been competitive with autoregressive large language models (LLMs). Recently,  LLaDA  showed comparable performance to Meta's Llama 2 7B but fell short of Llama 3 8B and other similarly sized modern LLMs.\n\nWhy it matters:  Text diffusion models are already faster than autoregressive models. They offer significant promise to accelerate text generation even further.\n\nWe're thinking:  Diffusion image generators have delivered good output with as little as four or even one step, generating output tokens significantly faster than autoregressive models. If text diffusion models can benefit from improvements in image generation, they could lead to rapid generation of lengthy texts and, in turn, faster agents and reasoning.""]"
