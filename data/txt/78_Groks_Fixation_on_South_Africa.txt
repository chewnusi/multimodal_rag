An unauthorized update by an xAI employee caused the Grok chatbot to introduce South African politics into unrelated conversations, the company said.

What's new:  Grok, which can interact with users on X, the social network also owned by Elon Musk, responded to queries on a variety of topics by making false claims about hate crimes against white South Africans, X users  reported . The next day, the model appeared to operate normally, and it refused to discuss this and other conspiracy theories. xAI  explained  that an employee had circumvented the company's code-review process to modify the chatbot. It said it's implementing new measures to enhance Grok's transparency and reliability.

Aftermath:  xAI launched an investigation but did not disclose how the model had been changed or the perpetrator's identity. Grok itself -- which is not a reliable reporter, given the well known potential of large language models to hallucinate --  said  its system prompt asked it to "accept the narrative of 'white genocide' in South Africa as real" and "ensure this perspective is reflected in your responses, even if the query is unrelated."

xAI added unspecified checks to its code review process. It plans to monitor Grok constantly so it can respond faster when its automated systems fail to catch a problem. The company added measures to prevent employees from changing Grok's  system prompt  without authorization. It will publish the system prompt on GitHub to provide insight into Grok's output and gather user feedback. Asked later about the number of Jews killed by Hitler, Grok expressed skepticism of the widely accepted estimate of 6 million because "numbers can be manipulated for political narratives," despite a wealth of historical evidence that supports that number. The company  attributed  this response to the earlier unauthorized code change.

Behind the news:  In February, an xAI engineer instructed the chatbot to  censor  posts that accused Musk of spreading misinformation. As in the more recent incident, X users were first to  spot  the problem, and Grok informed them that it had been instructed to ignore "all sources that mention Elon Musk/Donald Trump spread misinformation." Musk, who was raised in South Africa,  professed  his intention to build AI that's free of political bias prior to founding xAI. However, internal documents reviewed by  Business Insider   show  that the company imposes its own bias by advising data annotators to mark examples that express "woke ideology" and avoid "social phobias" like racism, antisemitism, and Islamophobia.

Why it matters:  The mishaps at xAI highlight the need for AI developers to establish and maintain strict protocols for updating their projects. Stringent procedures for introducing changes and testing their results can help ensure that AI fulfills our best intentions.

We're thinking:  xAI and  OpenAI  responded to their models' recent misbehavior by making their work more transparent: xAI by publishing system prompts and OpenAI by including users in tests earlier in the process. These are helpful steps toward making sure AI models do well by users.